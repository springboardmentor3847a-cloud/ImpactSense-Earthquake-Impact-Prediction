# ===============================================================
# üìò ImpactSense - Earthquake Impact Prediction
# üß© Milestone 3: Evaluation, Explainability & Reporting
# ===============================================================

# ----------------------------
# 1. Imports
# ----------------------------
import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import (
    confusion_matrix, classification_report, accuracy_score,
    precision_score, recall_score, f1_score
)
from sklearn.model_selection import train_test_split, learning_curve
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
import joblib

# Try importing SHAP for explainability
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("‚ö†Ô∏è SHAP not installed. Run `pip install shap` to enable explainability plots.")

# ----------------------------
# 2. Load Data (with Auto Detection)
# ----------------------------
PROCESSED_FILE = "earthquake_processed.csv"

if os.path.exists(PROCESSED_FILE):
    print(f"‚úÖ Found '{PROCESSED_FILE}'. Loading data...")
    df = pd.read_csv(PROCESSED_FILE)
else:
    print("‚ö†Ô∏è 'earthquake_processed.csv' not found. Creating sample dataset for testing...")
    from sklearn.datasets import make_classification
    X, y = make_classification(
        n_samples=500,
        n_features=6,
        n_informative=4,
        n_redundant=1,
        random_state=42
    )
    df = pd.DataFrame(X, columns=[f"feature_{i}" for i in range(6)])
    df["risk_level"] = np.random.choice(["Low", "Moderate", "High"], size=500)
    df.to_csv("earthquake_processed_sample.csv", index=False)
    print("üß© Sample dataset created as 'earthquake_processed_sample.csv'.")

print("‚úÖ Data loaded successfully.")
print("Shape:", df.shape)
print(df.head())

# ----------------------------
# 3. Prepare Features and Target
# ----------------------------
target_col = "risk_level" if "risk_level" in df.columns else df.columns[-1]
X = df.drop(columns=[target_col])
y = df[target_col]

# Encode target if categorical
if y.dtype == "object" or y.dtype.name == "category":
    le = LabelEncoder()
    y = le.fit_transform(y)
    print("üî§ Target encoded:", le.classes_)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split into train/test
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

print("‚úÖ Data split into training and test sets.")

# ----------------------------
# 4. Load or Train Model
# ----------------------------
MODEL_FILE = "Best_Earthquake_Model.pkl"

if os.path.exists(MODEL_FILE):
    print(f"‚úÖ Found model file '{MODEL_FILE}'. Loading...")
    model = joblib.load(MODEL_FILE)
else:
    print("‚ö†Ô∏è Model file not found. Training new Random Forest model...")
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    joblib.dump(model, MODEL_FILE)
    print("‚úÖ Model trained and saved as 'Best_Earthquake_Model.pkl'.")

# ----------------------------
# 5. Evaluation Metrics
# ----------------------------
y_pred = model.predict(X_test)

acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, average="weighted", zero_division=0)
rec = recall_score(y_test, y_pred, average="weighted", zero_division=0)
f1 = f1_score(y_test, y_pred, average="weighted", zero_division=0)

print("\nüìä Model Evaluation Metrics:")
print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1-score : {f1:.4f}")

# Detailed classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, zero_division=0))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# ----------------------------
# 6. Feature Importance
# ----------------------------
if hasattr(model, "feature_importances_"):
    importances = model.feature_importances_
    feat_df = pd.DataFrame({
        "Feature": X.columns,
        "Importance": importances
    }).sort_values(by="Importance", ascending=False)

    plt.figure(figsize=(8,5))
    sns.barplot(x="Importance", y="Feature", data=feat_df.head(10), palette="viridis")
    plt.title("Top 10 Important Features")
    plt.show()
else:
    print("‚ö†Ô∏è This model does not provide feature importances.")

# ----------------------------
# 7. SHAP Explainability (Optional)
# ----------------------------
if SHAP_AVAILABLE:
    print("\nüîç Running SHAP explainability...")
    shap.initjs()
    explainer = shap.TreeExplainer(model)
    X_sample = pd.DataFrame(X_test, columns=X.columns).sample(100, random_state=42)
    shap_values = explainer.shap_values(X_sample)

    # If multi-class, handle list output
    if isinstance(shap_values, list):
        for i, sv in enumerate(shap_values):
            print(f"üìà SHAP summary for class {i}")
            shap.summary_plot(sv, X_sample, show=True)
    else:
        shap.summary_plot(shap_values, X_sample, show=True)
else:
    print("‚ö†Ô∏è SHAP not available. Run `pip install shap` to enable explainability plots.")

# ----------------------------
# 8. Learning Curve
# ----------------------------
train_sizes, train_scores, test_scores = learning_curve(
    model, X_scaled, y, cv=5, scoring="accuracy", n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 5)
)

train_mean = np.mean(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)

plt.figure(figsize=(8,5))
plt.plot(train_sizes, train_mean, "o-", label="Training Accuracy")
plt.plot(train_sizes, test_mean, "o-", label="Cross-validation Accuracy")
plt.title("Learning Curve")
plt.xlabel("Training Examples")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

# ----------------------------
# 9. Misclassified Samples
# ----------------------------
mis_idx = np.where(y_test != y_pred)[0]
print(f"‚ö†Ô∏è Number of misclassified samples: {len(mis_idx)}")

if len(mis_idx) > 0:
    sample_err = min(10, len(mis_idx))
    err_df = pd.DataFrame(X_test[mis_idx[:sample_err]], columns=X.columns)
    err_df["True_Label"] = y_test[mis_idx[:sample_err]]
    err_df["Pred_Label"] = y_pred[mis_idx[:sample_err]]
    print("\nüîç Sample Misclassified Records:")
    print(err_df.head())

# ----------------------------
# 10. Save Evaluation Report
# ----------------------------
report = pd.DataFrame([{
    "Accuracy": acc,
    "Precision": prec,
    "Recall": rec,
    "F1_Score": f1,
    "Test_Samples": len(y_test)
}])
report.to_csv("Milestone3_Evaluation_Report.csv", index=False)
print("‚úÖ Evaluation results saved to 'Milestone3_Evaluation_Report.csv'")

# ----------------------------
# 11. Next Steps (Milestone 4)
# ----------------------------
print("\nüöÄ Next Steps for Milestone 4:")
print("1Ô∏è‚É£ Build a Streamlit or FastAPI UI for earthquake impact prediction")
print("2Ô∏è‚É£ Test UI inputs with different values and record outputs")
print("3Ô∏è‚É£ Prepare final presentation slides and summary report")
